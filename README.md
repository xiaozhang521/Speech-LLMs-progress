# Speech-LLMs-progress
Summarization of current Speech LLMs: the ranking of all the work is random.

 Paper | Name | Speech model | LLM | Method | Architecture
-----|----|------|-----|---|------|
JOINT AUDIO AND SPEECH UNDERSTANDING | LTU-AS | Wishper | LLaMA | For a speech, they apply a wishper encoder to get the logits. Then following a projection layer to convert it to text-level token. The next tokens are generated by the wishper decoder. By adding the corresponding instruction and LoRA adapter, they gained a deocder-only speech LLMs based on LLaMA. |![Alt text](image-1.png) 
LISTEN, THINK, AND UNDERSTAND | LTU |  Audio Spectrogram Transformer | LLaMA |The training stratergy is similar with the LTU-AS but use the speech encoder. Thus the training data can only be audio-text pairs. | ![Alt text](image-2.png)
Qwen-Audio: Advancing Universal Audio Understanding via Unified Large-Scale Audio-Language Models| Qwen-Audio | Whisper encoder | Qwen| This work apply the Whisper-style format to tag the audio, then requires the Qwen model to predict the tag message including task, time stamp, language, transcription and so on. | ![Alt text](image-3.png)
SALMONN: TOWARDS GENERIC HEARING ABILITIES FOR LARGE LANGUAGE MODELS | SALMONN |  Wishper encoder (speech) & BEATs encoder (Audio) | Vicuna | To porcess the audio, this work apply two speech encoder to model speech and audio sperately. The two feartrues were stacked and processed by Window-level Q-former. This method design a fixed feature space and compress information from window-size speech feature to the space. Then the feature produce by the Q-former can viewed as the speech token, after following proper instruction tuning with lora adpter, the LLMs can process the speech and sound.| ![Alt text](image-4.png)
Audio Flamingo: A Novel Audio Language Model with Few-Shot Learning and Dialogue Abilities |Audio Flamingo| ClapCap (Audio feature) |OPT-IML-MAX-1.3B|Strong audio understanding, benefit from in-context learning, multi-turn dialogue. It use a cross-attention and gated network to fuse the audio information into LLMs. | ![alt text](image-5.png)
UnIVAL: Unified Model for Image, Video, Audio and Language Tasks | UnIVAL | Modality-specific encoder(ResNet-101 for image, 3D ResNext-101 for video, PANN for audio) | BART-base | A middle size model (~0.25B) to process all types of modality, but it requires to finetune on the downstream task. To improve the training efficience, they apply quality data to avoid using massive data and design the multimodal curriculum learning. "The prevailing approach for pretraining multimodal models revolves around training them on large, noisy image-caption datasets". | ![alt text](image-6.png)
LLASM: LARGE LANGUAGE AND SPEECH MODEL |Llasm| Wishper encoder | Chinese-LLaMA2-7B | The method is similar with LLaVA. There are modal adapter to bridge the gap between speech features and word embeddings. The pretraining stage only adjust the adapter, then following the instruction tuning via multi-task learning.  The fine-tuning stage will update the LLMs and adapter, mainly utilizes multi-turn QA dataset. | ![alt text](image-7.png)
Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding|Video-LLaMA| Imagebind| LLaMA or Vicuna |Applying Q-former and ImageBind for extracting the temporal change information and audio-visual signal. The LLM and modality extractor are freezed.| ![alt text](image-8.png)
MACAW-LLM: MULTI-MODAL LANGUAGE MODELING WITH IMAGE, AUDIO, VIDEO, AND TEXT INTEGRATION |MACAW-LLM| CLIP(Image), Wishper encoder(Audio)|LLaMA-7B |Aim to integrate four modality feature (image, video, audio and text) to LLMs. They apply the conv1D to adjust the length of different modality features. The they use the cross attention to align the speech and image feature to word embdding. Then they directly start to the instraction tuneing rather applying a pretraining stage. |![alt text](image-9.png)
LauraGPT: Listen, Attend, Understand, and Regenerate Audio with GPT | Lauragpt| Conformer encoder and improved EnCodec | Qwen-1.8B | They apply continuous feature as input and discrete feature(codec) as output. They simplify the synthesiss process which convert the codec token to audio by one vocoder. | ![alt text](image-10.png)
MUSIC UNDERSTANDING LLAMA: ADVANCING TEXT-TO-MUSIC GENERATION WITH QUESTION ANSWERING AND CAPTIONING |MU-LLaMA|MERT encoder| LLaMA| They use MERT to conver the MUSIC to features and following an adapter. The output of adapter will be the query of attention in the last layer of LLM. The training progress only updates adpater parameters.|![alt text](image-11.png)
Pengi: An Audio Language Model for Audio Tasks |pengi| | | |![alt text](image-12.png)
|SLM|
|cosmic|
|next-gpt|
|anygpt|
|AudioPalm|
|SpeechGPT|
|AudioGPT|
|SEEDASR|
|FUNASRLLM|
|wavLLM|
|SPIRIT-LM|
|speech LLama|
|voxtLM|
|VALLE-X|
