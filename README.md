# Speech LLMs Progress
A summarization of current Speech LLMs, the ranking of the following works is random.

 Paper | Name | Speech model | LLM | Method | Architecture
-----|----|------|-----|---|------|
JOINT AUDIO AND SPEECH UNDERSTANDING | [LTU-AS](https://github.com/yuangongnd/ltu) | Wishper | LLaMA | For a speech, they apply a wishper encoder to get the logits. Then following a projection layer to convert it to text-level token. The next tokens are generated by the wishper decoder. By adding the corresponding instruction and LoRA adapter, they gained a deocder-only speech LLMs based on LLaMA. |![Alt text](image-1.png) 
LISTEN, THINK, AND UNDERSTAND | [LTU](https://github.com/yuangongnd/ltu) |  Audio Spectrogram Transformer | LLaMA |The training stratergy is similar with the LTU-AS but use the speech encoder. Thus the training data can only be audio-text pairs. | ![Alt text](image-2.png)
Qwen-Audio: Advancing Universal Audio Understanding via Unified Large-Scale Audio-Language Models| Qwen-Audio | Whisper encoder | Qwen| This work apply the Whisper-style format to tag the audio, then requires the Qwen model to predict the tag message including task, time stamp, language, transcription and so on. | ![Alt text](image-3.png)
SALMONN: TOWARDS GENERIC HEARING ABILITIES FOR LARGE LANGUAGE MODELS | [SALMONN](https://github.com/bytedance/SALMONN) |  Wishper encoder (speech) & BEATs encoder (Audio) | Vicuna | To porcess the audio, this work apply two speech encoder to model speech and audio sperately. The two feartrues were stacked and processed by Window-level Q-former. This method design a fixed feature space and compress information from window-size speech feature to the space. Then the feature produce by the Q-former can viewed as the speech token, after following proper instruction tuning with lora adpter, the LLMs can process the speech and sound.| ![Alt text](image-4.png)
Audio Flamingo: A Novel Audio Language Model with Few-Shot Learning and Dialogue Abilities |[Audio Flamingo](https://github.com/NVIDIA/audio-flamingo)| ClapCap (Audio feature) |OPT-IML-MAX-1.3B|Strong audio understanding, benefit from in-context learning, multi-turn dialogue. It use a cross-attention and gated network to fuse the audio information into LLMs. | ![alt text](image-5.png)
UnIVAL: Unified Model for Image, Video, Audio and Language Tasks | [UnIVAL](https://github.com/mshukor/UnIVAL) | Modality-specific encoder(ResNet-101 for image, 3D ResNext-101 for video, PANN for audio) | BART-base | A middle size model (~0.25B) to process all types of modality, but it requires to finetune on the downstream task. To improve the training efficience, they apply quality data to avoid using massive data and design the multimodal curriculum learning. | ![alt text](image-6.png)
LLASM: LARGE LANGUAGE AND SPEECH MODEL |[Llasm](https://github.com/LinkSoul-AI/LLaSM)| Wishper encoder | Chinese-LLaMA2-7B | The method is similar with LLaVA. There are modal adapter to bridge the gap between speech features and word embeddings. The pretraining stage only adjust the adapter, then following the instruction tuning via multi-task learning.  The fine-tuning stage will update the LLMs and adapter, mainly utilizes multi-turn QA dataset. | ![alt text](image-7.png)
Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding|[Video-LLaMA](https://github.com/DAMO-NLP-SG/Video-LLaMA)| Imagebind| LLaMA or Vicuna |Applying Q-former and ImageBind for extracting the temporal change information and audio-visual signal. The LLM and modality extractor are freezed.| ![alt text](image-8.png)
MACAW-LLM: MULTI-MODAL LANGUAGE MODELING WITH IMAGE, AUDIO, VIDEO, AND TEXT INTEGRATION |[MACAW-LLM](https://github.com/lyuchenyang/Macaw-LLM)| CLIP(Image), Wishper encoder(Audio)|LLaMA-7B |Aim to integrate four modality feature (image, video, audio and text) to LLMs. They apply the conv1D to adjust the length of different modality features. The they use the cross attention to align the speech and image feature to word embdding. Then they directly start to the instraction tuneing rather applying a pretraining stage. |![alt text](image-9.png)
LauraGPT: Listen, Attend, Understand, and Regenerate Audio with GPT | [Lauragpt](https://lauragpt.github.io/)| Conformer encoder and improved EnCodec | Qwen-1.8B | They apply continuous feature as input and discrete feature(codec) as output. They simplify the synthesiss process which convert the codec token to audio by one vocoder. | ![alt text](image-10.png)
MUSIC UNDERSTANDING LLAMA: ADVANCING TEXT-TO-MUSIC GENERATION WITH QUESTION ANSWERING AND CAPTIONING |[MU-LLaMA](https://github.com/shansongliu/MU-LLaMA)|MERT encoder| LLaMA| They use MERT to conver the MUSIC to features and following an adapter. The output of adapter will be the query of attention in the last layer of LLM. The training progress only updates adpater parameters.|![alt text](image-11.png)
Pengi: An Audio Language Model for Audio Tasks |[Pengi](https://github.com/microsoft/Pengi)|CLAP, a frozen encoder to model text prompt |GPT2-base | They apply two encoder to process the audio and text sperately, then two mapping network convert the features to fixed-length. The CLAP and mapping networks are updated. |![alt text](image-12.png)
SLM: BRIDGE THE THIN GAP BETWEEN SPEECH AND TEXT FOUNDATION MODELS |[SLM](https://arxiv.org/pdf/2310.00230)| Encoder of USM | T5-13B(mT0-MT XXL) | Effcient tuning which only update the adapter to bridge the gap between speech encoder and LLMs. |![alt text](image-13.png)
COSMIC: Data Efficient Instruction-tuning For Speech In-Context Learning|[COSMIC](https://arxiv.org/pdf/2311.02248)|Whisper encoder |LLaMA-2|Improve the in-context learning ability by concate different samples as instructon to tune model. The speech samples are convert to features wiht the Q-former fashion.|![alt text](image-14.png)
NExT-GPT: Any-to-Any Multimodal LLM|[NeXT-gpt](https://next-gpt.github.io/)|Imagebind | Vicuna-7B| Make the LLMs understand all the modalities (text, image, video, and audio), using modal concept token to guide LLMs process extracted feature. They apply the modality-switching instruction tuning which achieve the text to other modality generation.| ![alt text](image-15.png)
AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling|[Anygpt](https://junzhan2000.github.io/AnyGPT.github.io/)|SpeechTokenizer  (VQ-VAE) for speech, SEED (ViT+Q-former) for image,  Encodec for music |LLaMA-2 (7B)| To make the LLMs process speech, text, images, and music, they design a strategy to build AnyInstruct-108k, a multi-modal multi-turn dataset. The pipeline of data product is: 1. obtaining the text about the topic,2. generating more details about the scenarios, 3. descriping more chat information which to inject the multi-modal data, 4. synthesis the other modal data. The previous three steps are finished with help of GPT-4. They apply DALLE-3, MusicGen and Microsof Azure to product image, music and speech seperately. |![alt text](image-16.png)
AudioPaLM: A Large Language Model That Can Speak and Listen|[AudioPalm](https://google-research.github.io/seanet/audiopalm/examples/)|SoundStream|PaLM|Simply convert the speech to codec and apply the auto-regressive training for concanted text and codec. |![alt text](image-17.png)
SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities|[SpeechGPT](https://github.com/0nutation/SpeechGPT)| HuBERT | LLaMA| Convert the speech to unit tokens, then similarly with text auto-regressive pretraining to build the speech LLMs. |![alt text](image-18.png)
AudioGPT: Understanding and Generating Speech, Music, Sound, and Talking Head|[AudioGPT](https://github.com/AIGC-Audio/AudioGPT)| A series of models |gpt-3.5-turbo | A set of different models including LLMs to process audio tasks. It consists of four steps: 1) Modality transformation, 2) Task analysis, 3) Model assignment, 4) Response generation. |![alt text](image-19.png)
Seed-ASR: Understanding Diverse Speech and Contexts with LLM-based Speech Recognition|[SEED-ASR](https://arxiv.org/html/2407.04675v1)|LUISE (seems an improved HuBERT) |-|Focus on multi-lingual close-ended speech task, e.g., ASR, ST. |![alt text](image-20.png)
WavLLM: Towards Robust and Adaptive Speech Large Language Model|wavLLM|Whisper encoder and WavLM | LLaMA-2-7B-chat| They apply two audio encoder to extract textual and acoustic feature seperately. There are three adaptors for semantic, acoustic and prompt module. The other parts excepts Lora are frozed during the training progress. For curriculum Learning, they first train the single task then follow multi-task to avoid the LLM overfits on several specific speech tasks. |![alt text](image-23.png)
SPIRIT-LM: Interleaved Spoken and Written Language Model|[SPIRIT-LM](https://speechbot.github.io/spiritlm)| HuBERT for semantic token, VQ-VAE for pitch token, SONAR for style token |LLaMA-2-7B|Mix-up the speech and text tokens to achieve modality alignment. Additional style and pitch tokens extracted from speech. | ![alt text](image-24.png)
ON DECODER-ONLY ARCHITECTURE FOR SPEECH-TO-TEXT AND LARGE LANGUAGE MODEL INTEGRATION|[Speech LLaMA](https://arxiv.org/pdf/2307.03917)| Wishper encoder|LLaMA-7B|Apply a CTC to compress the speech sequence, then following a audio encoder which is a 4-layer Transformer. The training is auto-regressive based on LoRA.| ![alt text](image-25.png)
SpeechVerse: A Large-scale Generalizable Audio Language Model | [SpeechVerse](https://arxiv.org/pdf/2405.08295)| WavLM-Large|Flan-T5-XL|They sample the instruction to balance different tasks and apply curriculum learning to improve training efficiency. The model supports open-ended task rather specific tasks.|![alt text](image-27.png)
VOXTLM: UNIFIED DECODER-ONLY MODELS FOR CONSOLIDATING SPEECH RECOGNITION, SYNTHESIS AND SPEECH, TEXT CONTINUATION TASKS|[VOXTLM](https://soumimaiti.github.io/icassp24_voxtlm/)|HuBERT|OPT|Unified vocabulary for speech codec and text tokens.|![alt text](image-26.png)

