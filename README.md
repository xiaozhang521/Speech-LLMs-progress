# Speech-LLMs-progress
Summarization of current Speech LLMs: the ranking of all the work is random.

 Paper | Model | Speech model | LLM | Method | Architecture
-----|----|------|-----|---|------|
JOINT AUDIO AND SPEECH UNDERSTANDING | LTU-AS | Wishper | LLaMA | For a speech, they apply a wishper encoder to get the logits. Then following a projection layer to convert it to text-level token. The next tokens are generated by the wishper decoder. By adding the corresponding instruction and LoRA adapter, they gained a deocder-only speech LLMs based on LLaMA. |![Alt text](image-1.png) 
LISTEN, THINK, AND UNDERSTAND | LTU | LLaMA | Audio Spectrogram Transformer | The training stratergy is similar with the LTU-AS but use the speech encoder. Thus the training data can only be audio-text pairs. | ![Alt text](image-2.png)
Qwen-Audio: Advancing Universal Audio Understanding via Unified Large-Scale Audio-Language Models| Qwen-Audio | Whisper encoder | Qwen| This work apply the Whisper-style format to tag the audio, then requires the Qwen model to predict the tag message including task, time stamp, language, transcription and so on. | ![Alt text](image-3.png)
SALMONN: TOWARDS GENERIC HEARING ABILITIES FOR LARGE LANGUAGE MODELS | SALMONN |  Wishper encoder (speech) & BEATs encoder (Audio) | Vicuna | To porcess the audio, this work apply two speech encoder to model speech and audio sperately. The two feartrues were stacked and processed by Window-level Q-former. This method design a fixed feature space and compress information from window-size speech feature to the space. Then the feature produce by the Q-former can viewed as the speech token, after following proper instruction tuning with lora adpter, the LLMs can process the speech and sound.| ![Alt text](image-4.png)
Audio Flamingo: A Novel Audio Language Model with Few-Shot Learning and Dialogue Abilities |Audio Flamingo| ClapCap (Audio feature) |OPT-IML-MAX-1.3B|Strong audio understanding, benefit from in-context learning, multi-turn dialogue. It use a cross-attention and gated network to fuse the audio information into LLMs. | ![alt text](image-5.png)
UnIVAL: Unified Model for Image, Video, Audio and Language Tasks | UnIVAL | Modality-specific encoder(ResNet-101 for image, 3D ResNext-101 for video, PANN for audio) | BART-base | A middle size model (~0.25B) to process all types of modality, but it requires to finetune on the downstream task. To improve the training efficience, they apply quality data to avoid using massive data and design the multimodal curriculum learning. "The prevailing approach for pretraining multimodal models revolves around training them on large, noisy image-caption datasets". | ![alt text](image-6.png)
LLASM: LARGE LANGUAGE AND SPEECH MODEL |Llasm| Wishper encoder | Chinese-LLaMA2-7B | The method is similar with LLaVA. There are modal adapter to bridge the gap between speech features and word embeddings. The pretraining stage only adjust the adapter, then following the instruction tuning via multi-task learning.  The fine-tuning stage will update the LLMs and adapter, mainly utilizes multi-turn QA dataset. | ![alt text](image-7.png)
|video-llama|
|Macaw-llm|
|Lauragpt|
|MU-LLaMA|
|pengi|
|SLM|
|cosmic|
|next-gpt|
|anygpt|
|AudioPalm|
|SpeechGPT|
|AudioGPT|
|SEEDASR|
|FUNASRLLM|
|wavLLM|
|SPIRIT-LM|
|speech LLama|
|voxtLM|
|VALLE-X|
